# RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy

**作者：** Zhonghan Zhao, Wenwei Zhang, Haian Huang, Kuikun Liu, Jianfei Gao, Gaoang Wang, Kai Chen

**发表日期：** 2025-03-31

**PDF链接：** [http://arxiv.org/pdf/2503.24388v1](http://arxiv.org/pdf/2503.24388v1)

**摘要内容：**

### 研究背景与目的
在复杂的开放世界环境中，理性思考和想象潜在结果对于操作的具身代理至关重要。然而，以往的工作要么仅在端到端代理中融合其中一种能力，要么将多个专门化模型整合到代理系统中，限制了政策的学习效率和泛化能力。因此，本文首次尝试在端到端的通用政策中协同推理和想象，称为RIG。

### 方法或技术要点
为了以端到端方式训练RIG，构建了一个数据管道，逐渐整合和丰富从现有代理收集的轨迹中想象和推理的内容。推理和下一图像生成的联合学习明确地建模了推理、行动和环境动态之间的固有相关性，因此在学习效率和泛化方面展现出超过17倍的样本效率改进。在推断过程中，RIG首先推理下一步动作，产生潜在动作，然后预测动作结果，为代理提供了在采取实际行动之前基于想象进行审查和自我纠正的机会。

### 实验结果或发现
实验结果表明，推理和想象的协同作用不仅提高了通用政策的鲁棒性、泛化能力和互操作性，还实现了测试时的扩展，增强了整体性能。

### 潜在应用场景
该研究成果有望在复杂开放环境中的智能代理系统中应用，提高政策的学习效率、泛化能力和鲁棒性，同时可以用于增强整体性能和测试时扩展。

---

# UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving

**作者：** Yuping Wang, Xiangyu Huang, Xiaokang Sun, Mingxuan Yan, Shuo Xing, Zhengzhong Tu, Jiachen Li

**发表日期：** 2025-03-31

**PDF链接：** [http://arxiv.org/pdf/2503.24381v1](http://arxiv.org/pdf/2503.24381v1)

**摘要内容：**

### 研究背景与目的
该论文旨在提出一个统一的基准测试集UniOcc，用于自动驾驶中的占用预测和预测，整合了真实世界数据集和高保真度驾驶模拟器数据，以提高占用预测和预测性能评估的准确性。

### 方法或技术要点
1. 将来自多个真实世界数据集和高保真度驾驶模拟器的数据整合到UniOcc中，提供了2D/3D占用标签和每个体素流注释。
2. 引入了新颖的评估指标，不依赖于地面真实占用情况，能够更全面地评估占用质量。
3. 通过在最先进模型上进行大量实验，证明大规模、多样化的训练数据和明确的流信息显著增强了占用预测和预测性能。

### 实验结果或发现
实验表明，UniOcc提供的数据集和评估指标能够有效提高占用预测和预测的性能。大规模、多样化的训练数据以及明确的流信息对提升性能起到关键作用。

### 潜在应用场景
该研究成果可应用于自动驾驶系统中，提高对车辆周围环境的占用预测和预测准确性，进一步增强自动驾驶车辆的安全性和协作性能。

---

# Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1

**作者：** Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, Xihui Liu

**发表日期：** 2025-03-31

**PDF链接：** [http://arxiv.org/pdf/2503.24376v1](http://arxiv.org/pdf/2503.24376v1)

**摘要内容：**

### 研究背景与目的
- 最近，大型语言模型（LLMs）在推理能力方面取得了显著进展，强化学习（RL）作为一种有效的后训练方法备受关注。然而，在需要同时具备感知和逻辑推理能力的任务中，多模态大型语言模型（MLLMs）仍未得到充分探索。
- 为了解决这一问题，引入了SEED-Bench-R1基准测试，旨在系统评估MLLMs在视频理解中的后训练方法。

### 方法或技术要点
- SEED-Bench-R1基准测试包括真实世界视频和复杂的日常规划任务，以多项选择题的形式呈现，需要复杂的感知和推理能力。
- 基于Qwen2-VL-Instruct-7B作为基础模型，比较了RL和监督微调（SFT），结果显示RL在数据效率和性能上均优于SFT，甚至在LongVideoBench等视频理解基准测试中表现更好。
- 详细分析显示，RL增强了视觉感知，但常常产生逻辑推理链条较不连贯的问题。

### 实验结果或发现
- RL在内部分布和跨环境任务中表现出色，甚至在LongVideoBench等视频理解基准测试中也表现优异。
- 研究发现RL增强了视觉感知，但推理链条常不够连贯，存在推理不一致和忽视视觉线索等关键限制。

### 潜在应用场景
- 未来可改进基础模型推理、奖励建模和RL对嘈杂信号的鲁棒性，进一步提高MLLMs在视频理解任务中的性能和效率。
  
通过SEED-Bench-R1基准测试的研究，揭示了RL在视频理解中的潜力和局限性，为提升MLLMs在复杂任务中的应用提供了重要启示。

---

# Policy Gradient for LQR with Domain Randomization

**作者：** Tesshu Fujinami, Bruce D. Lee, Nikolai Matni, George J. Pappas

**发表日期：** 2025-03-31

**PDF链接：** [http://arxiv.org/pdf/2503.24371v1](http://arxiv.org/pdf/2503.24371v1)

**摘要内容：**

### 研究背景与目的
- 研究背景：域随机化（DR）通过在模拟环境分布上训练控制器，实现模拟到真实世界的迁移，以在真实世界中获得稳健的性能。
- 研究目的：探讨域随机化线性二次调节（LQR）问题中简单策略梯度（PG）方法的收敛性，并提供理论保证。

### 方法或技术要点
- 提供了PG方法在域随机化LQR问题中的全局收敛性分析。
- 展示PG在适当的采样系统异质性边界下，全局收敛于DR目标的有限样本逼近的最小化器。
- 量化了实现样本平均值和总体水平目标之间小性能差距所需的样本复杂度。
- 提出并分析了一种折扣因子退火算法，无需初始联合稳定控制器，从而克服了寻找难度大的问题。
- 实验证实了理论结果，并指出了未来工作的方向，包括风险敏感的DR公式和随机PG算法。

### 实验结果或发现
- 理论结果得到了实验证实的支持。
- 发现未来工作方向，包括风险敏感的DR公式和随机PG算法。

### 潜在应用场景
- 该研究可应用于域随机化控制器设计，有助于实现模拟环境到真实世界的稳健性能转移。
- 可为机器人控制、自动驾驶等领域提供理论指导和实践支持。

---

# Effectively Controlling Reasoning Models through Thinking Intervention

**作者：** Tong Wu, Chong Xiang, Jiachen T. Wang, Prateek Mittal

**发表日期：** 2025-03-31

**PDF链接：** [http://arxiv.org/pdf/2503.24370v1](http://arxiv.org/pdf/2503.24370v1)

**摘要内容：**

### 研究背景与目的
砠究背景：大型语言模型（LLMs）在生成最终答案之前明确生成中间推理步骤，帮助模型在复杂问题解决中表现出色。
研究目的：探讨新兴生成框架在更精细控制模型行为方面的独特机会，提出Thinking Intervention范式，旨在通过策略性地插入或修订特定的思考标记来明确指导LLMs的内部推理过程。

### 方法或技术要点
- 提出Thinking Intervention范式，通过插入或修订思考标记来引导LLMs的内部推理过程。
- 在多个任务上进行全面评估，包括IFEval上的指令遵循、SEP上的指令层次结构以及XSTest和SORRY-Bench上的安全对齐。
- 使用开源DeepSeek R1模型，实现了指令遵循场景中高达6.7%的准确率提升，指令层次推理中15.4%的改进，以及对不安全提示的拒绝率提高40.0%。

### 实验结果或发现
Thinking Intervention明显优于基准提示方法，在多项任务中取得显著成果，表现为指令遵循场景中高达6.7%的准确率提升，指令层次推理中15.4%的改进，以及对不安全提示的拒绝率提高40.0%。

### 潜在应用场景
- 控制推理型LLMs的新研究方向。
- 在复杂问题解决、指令遵循和安全对齐等领域中，提供更精细的模型行为控制方法。

---

